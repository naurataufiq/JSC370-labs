knitr::opts_chunk$set(eval = F, include  = T)
install.packages(c("tidyverse", "tidytext", "wordcloud2", "topicmodels"))
install.packages("tm", dependencies=TRUE)
install.packages("stopwords")
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)
library(stopwords)
mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
select(description, medical_specialty, transcription)
head(mt_samples)
mt_samples |>
count(medical_specialty, sort = TRUE) |>
ggplot(aes(x = reorder(medical_specialty, n), y = n)) +
geom_col(fill = "purple") +
coord_flip() +
labs(
title = "Distribution of Medical Specialties in Transcriptions",
x = "Medical Specialty",
y = "Number of Transcriptions"
) +
theme_minimal()
mt_samples |>
distinct(medical_specialty) |>
nrow()
tokens <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
count(word, sort = TRUE)
tokens |>
slice_max(n, n = 20) |>
ggplot(aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity", fill = "purple") +
coord_flip() +
labs(
title = "Top 20 Most Frequent Words in Medical Transcriptions",
x = "Word",
y = "Frequency"
) +
theme_minimal()
tokens |>
slice_max(n, n = 20) |>
wordcloud2()
head(stopwords("english"))
length(stopwords("english"))
head(stop_words)
custom_stopwords <- c("patient", "history", "medical", "normal", "left", "mm", "mg", "pain")
tokens <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
filter(!word %in% stop_words$word) |>  # Remove common stopwords
filter(!word %in% stopwords("english")) |>  # Remove additional stopwords
filter(!str_detect(word, "^[0-9]+$")) |>  # Remove numbers
filter(!word %in% custom_stopwords) |>
count(word, sort = TRUE)
tokens |>
slice_max(n, n = 20) |>
ggplot(aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity", fill = "purple") +
coord_flip() +
labs(
title = "Top 20 Words (Stopwords & Numbers Removed)",
x = "Word",
y = "Frequency"
) +
theme_minimal()
tokens |>
slice_max(n, n = 20) |>
wordcloud2()
stopwords2 <- c(stopwords::stopwords("en"), custom_stopwords)
sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")
tokens_bigram <- mt_samples |>
select(transcription) |>
unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
separate(ngram, into = c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stopwords2, !word2 %in% stopwords2) |>
unite(ngram, word1, word2, sep = " ") |>
count(ngram, sort = TRUE)
tokens_bigram |>
slice_max(n, n = 20) |>
ggplot(aes(x = reorder(ngram, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 20 Most Frequent Bigrams", x = "Bigram", y = "Count") +
theme_minimal()
# trigrams
tokens_trigram <- mt_samples |>
select(transcription) |>
unnest_tokens(ngram, transcription, token = "ngrams", n = 3) |>
separate(ngram, into = c("word1", "word2", "word3"), sep = " ") |>
filter(!word1 %in% stopwords2, !word2 %in% stopwords2, !word3 %in% stopwords2) |>
unite(ngram, word1, word2, word3, sep = " ") |>
count(ngram, sort = TRUE)
tokens_trigram |>
slice_max(n, n = 20) |>
ggplot(aes(x = reorder(ngram, n), y = n)) +
geom_col(fill = "darkred") +
coord_flip() +
labs(title = "Top 20 Most Frequent Trigrams", x = "Trigram", y = "Count") +
theme_minimal()
